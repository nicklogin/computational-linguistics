{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Задание 1 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим правила для анализа предложений: <br /><br />\n",
    "    1) Вася читает мою книгу <br />\n",
    "    2) Напиши какое-нибудь письмо <br />\n",
    "    3) Этот веселый мальчик идет <br />\n",
    "    4) Он любит читать всякие книги.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = \"\"\"\n",
    "    S -> NP VP | VP\n",
    "    NP -> Det Adj N | Det N | NN\n",
    "    VP -> V VP | V NP | V\n",
    "    Det -> 'мою' | 'какое-нибудь' | 'этот' | 'всякие'\n",
    "    Adj -> 'весёлый'\n",
    "    N -> 'мальчик' | 'книгу' | 'письмо' | 'книги'\n",
    "    NN -> 'Вася'\n",
    "    V -> 'читает' | 'Напиши' | 'идёт' | 'любит' | 'читать'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем анализ предложения при помощи Алгоритма CYK:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr><td>S</td><td></td><td></td><td></td></tr>\n",
    "    <tr><td></td><td>VP</td><td></td><td></td></tr>\n",
    "    <tr><td>NP</td><td></td><td>NP</td><td></td></tr>\n",
    "    <tr><td>NN</td><td>V</td><td>Det</td><td>N</td></tr>\n",
    "    <tr><td>Вася</td><td>читает</td><td>мою</td><td>книгу</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сработавшие правила: <br />\n",
    "NN -> 'Вася' <br />\n",
    "V -> 'читает' <br />\n",
    "Det -> 'мою' <br />\n",
    "N -> 'книгу' <br />\n",
    "NP -> NN <br />\n",
    "NP -> Det N <br />\n",
    "VP -> V NP <br />\n",
    "S -> NP VP <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также опишем его анализ при помощи Алгоритма Эрли:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. <br />\n",
    "$\\gamma$ -> . S [0,0] Dummy start state <br />\n",
    "S -> . NP VP [0,0] Predictor <br />\n",
    "NP -> . Det Adj N [0,0] Predictor <br />\n",
    "NP -> . Det N [0,0] Predictor <br />\n",
    "NP -> . NN [0,0] Predictor <br />\n",
    "S -> .VP [0,0] Predictor <br />\n",
    "VP -> .V VP [0,0] Predictor <br />\n",
    "VP -> . V NP [0,0] Predictor <br />\n",
    "VP -> . V [0,0] Predictor <br />\n",
    "\n",
    "1. <br />\n",
    "NN -> Вася . [0,1] Scanner <br />\n",
    "NP -> NN . [0,1] Completer <br />\n",
    "S -> NP . VP [0,1] Completer <br />\n",
    "VP -> .V VP [1,1] Predictor <br />\n",
    "VP -> . V NP [1,1] Predictor <br />\n",
    "VP -> . V [1,1] Predictor <br />\n",
    "\n",
    "2. <br />\n",
    "V -> Читает . [1,2] Scanner <br />\n",
    "VP -> V . [1,2] Completer <br />\n",
    "VP -> V. VP [1, 2] Completer <br />\n",
    "VP -> V . NP [2,2] Completer <br />\n",
    "VP -> . V [2,2] Predictor <br />\n",
    "VP -> . V VP [2,2] Predictor <br />\n",
    "VP -> . V NP [2,2] Predictor <br />\n",
    "NP -> . Det Adj N [2,2] Predictor <br />\n",
    "NP -> . Det N [2,2] Predictor <br />\n",
    "NP -> . NN [2,2] Predictor <br />\n",
    "\n",
    "3. <br />\n",
    "Det -> мою . [2,3] Scanner <br />\n",
    "NP -> Det . N [2,3] Completer <br />\n",
    "NP -> Det . Adj N [2,3] Completer <br />\n",
    "\n",
    "4. <br />\n",
    "N -> книгу . [3,4] Scanner <br />\n",
    "NP -> Det N . [2,4] Completer <br />\n",
    "VP -> V NP . [1,4] Completer <br />\n",
    "S -> NP VP . [0,4] Completer <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Задание 2</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.   Вася  .  читает .   мою   .  книгу  .|\n",
      "|[---------]         .         .         .| [0:1] 'Вася'\n",
      "|.         [---------]         .         .| [1:2] 'читает'\n",
      "|.         .         [---------]         .| [2:3] 'мою'\n",
      "|.         .         .         [---------]| [3:4] 'книгу'\n",
      "|>         .         .         .         .| [0:0] S  -> * NP VP\n",
      "|>         .         .         .         .| [0:0] S  -> * VP\n",
      "|>         .         .         .         .| [0:0] VP -> * V VP\n",
      "|>         .         .         .         .| [0:0] VP -> * V NP\n",
      "|>         .         .         .         .| [0:0] VP -> * V\n",
      "|>         .         .         .         .| [0:0] NP -> * Det Adj N\n",
      "|>         .         .         .         .| [0:0] NP -> * Det N\n",
      "|>         .         .         .         .| [0:0] NP -> * NN\n",
      "|>         .         .         .         .| [0:0] NN -> * 'Вася'\n",
      "|[---------]         .         .         .| [0:1] NN -> 'Вася' *\n",
      "|[---------]         .         .         .| [0:1] NP -> NN *\n",
      "|[--------->         .         .         .| [0:1] S  -> NP * VP\n",
      "|.         >         .         .         .| [1:1] VP -> * V VP\n",
      "|.         >         .         .         .| [1:1] VP -> * V NP\n",
      "|.         >         .         .         .| [1:1] VP -> * V\n",
      "|.         >         .         .         .| [1:1] V  -> * 'читает'\n",
      "|.         [---------]         .         .| [1:2] V  -> 'читает' *\n",
      "|.         [--------->         .         .| [1:2] VP -> V * VP\n",
      "|.         [--------->         .         .| [1:2] VP -> V * NP\n",
      "|.         [---------]         .         .| [1:2] VP -> V *\n",
      "|[-------------------]         .         .| [0:2] S  -> NP VP *\n",
      "|.         .         >         .         .| [2:2] NP -> * Det Adj N\n",
      "|.         .         >         .         .| [2:2] NP -> * Det N\n",
      "|.         .         >         .         .| [2:2] NP -> * NN\n",
      "|.         .         >         .         .| [2:2] Det -> * 'мою'\n",
      "|.         .         >         .         .| [2:2] VP -> * V VP\n",
      "|.         .         >         .         .| [2:2] VP -> * V NP\n",
      "|.         .         >         .         .| [2:2] VP -> * V\n",
      "|.         .         [---------]         .| [2:3] Det -> 'мою' *\n",
      "|.         .         [--------->         .| [2:3] NP -> Det * Adj N\n",
      "|.         .         [--------->         .| [2:3] NP -> Det * N\n",
      "|.         .         .         >         .| [3:3] N  -> * 'книгу'\n",
      "|.         .         .         [---------]| [3:4] N  -> 'книгу' *\n",
      "|.         .         [-------------------]| [2:4] NP -> Det N *\n",
      "|.         [-----------------------------]| [1:4] VP -> V NP *\n",
      "|[=======================================]| [0:4] S  -> NP VP *\n",
      "(S (NP (NN Вася)) (VP (V читает) (NP (Det мою) (N книгу))))\n"
     ]
    }
   ],
   "source": [
    "grammar = nltk.CFG.fromstring(rules)\n",
    "parser = nltk.EarleyChartParser(grammar, trace=1)\n",
    "for tree in parser.parse(\"Вася читает мою книгу\".split()):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Задание 3. </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы улучшить работу этого парсера с новыми словами будем использовать морфологический тэггер, определять часть речи и создавать новое правила перехода определённой части речи в новое слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\k1l77\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]     C:\\Users\\k1l77\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_ru is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = lambda x: pos_tag(x, lang=\"rus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_parse(textgrammar, sent, tagger, tokenizer):\n",
    "    textgrammar = textgrammar.splitlines()\n",
    "    terminals = [rule.split('->')[1].strip(\" '\") for rule in textgrammar if \"-> '\" in rule]\n",
    "    sent = tagger(tokenizer(sent))\n",
    "    for token, tag in sent:\n",
    "        if token not in terminals:\n",
    "            textgrammar.append(\"    \"+tag+\" -> '\"+token+\"'\")\n",
    "    textgrammar = '\\n'.join(textgrammar)\n",
    "    print(textgrammar)\n",
    "    textgrammar = nltk.CFG.fromstring(textgrammar)\n",
    "    p = nltk.EarleyChartParser(textgrammar, trace=1)\n",
    "    for tree in p.parse([i[0] for i in sent]):\n",
    "        print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    S -> NP VP | VP\n",
      "    NP -> Det Adj N | Det N | NN\n",
      "    VP -> V VP | V NP | V\n",
      "    Det -> 'мою' | 'какое-нибудь' | 'этот' | 'всякие'\n",
      "    Adj -> 'весёлый'\n",
      "    N -> 'мальчик' | 'книгу' | 'письмо' | 'книги'\n",
      "    NN -> 'Вася'\n",
      "    V -> 'читает' | 'Напиши' | 'идёт' | 'любит' | 'читать'\n",
      "    V -> 'бьёт'\n",
      "    S -> 'Петю'\n",
      "|.    Вася   .    бьёт   .    Петю   .|\n",
      "|[-----------]           .           .| [0:1] 'Вася'\n",
      "|.           [-----------]           .| [1:2] 'бьёт'\n",
      "|.           .           [-----------]| [2:3] 'Петю'\n",
      "|>           .           .           .| [0:0] S  -> * NP VP\n",
      "|>           .           .           .| [0:0] S  -> * VP\n",
      "|>           .           .           .| [0:0] S  -> * 'Петю'\n",
      "|>           .           .           .| [0:0] VP -> * V VP\n",
      "|>           .           .           .| [0:0] VP -> * V NP\n",
      "|>           .           .           .| [0:0] VP -> * V\n",
      "|>           .           .           .| [0:0] NP -> * Det Adj N\n",
      "|>           .           .           .| [0:0] NP -> * Det N\n",
      "|>           .           .           .| [0:0] NP -> * NN\n",
      "|>           .           .           .| [0:0] NN -> * 'Вася'\n",
      "|[-----------]           .           .| [0:1] NN -> 'Вася' *\n",
      "|[-----------]           .           .| [0:1] NP -> NN *\n",
      "|[----------->           .           .| [0:1] S  -> NP * VP\n",
      "|.           >           .           .| [1:1] VP -> * V VP\n",
      "|.           >           .           .| [1:1] VP -> * V NP\n",
      "|.           >           .           .| [1:1] VP -> * V\n",
      "|.           >           .           .| [1:1] V  -> * 'бьёт'\n",
      "|.           [-----------]           .| [1:2] V  -> 'бьёт' *\n",
      "|.           [----------->           .| [1:2] VP -> V * VP\n",
      "|.           [----------->           .| [1:2] VP -> V * NP\n",
      "|.           [-----------]           .| [1:2] VP -> V *\n",
      "|[-----------------------]           .| [0:2] S  -> NP VP *\n",
      "|.           .           >           .| [2:2] NP -> * Det Adj N\n",
      "|.           .           >           .| [2:2] NP -> * Det N\n",
      "|.           .           >           .| [2:2] NP -> * NN\n",
      "|.           .           >           .| [2:2] VP -> * V VP\n",
      "|.           .           >           .| [2:2] VP -> * V NP\n",
      "|.           .           >           .| [2:2] VP -> * V\n"
     ]
    }
   ],
   "source": [
    "enhanced_parse(rules, \"Вася бьёт Петю\", tagger, word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсер не срабатывает правильно из-за неправильной работы POS-теггера!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
