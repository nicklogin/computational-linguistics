{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Confidence', 'NN'), ('in', 'IN'), ('the', 'DT'), ('pound', 'NN'), ('is', 'VBZ'), ('widely', 'RB'), ('expected', 'VBN'), ('to', 'TO'), ('take', 'VB'), ('another', 'DT'), ('sharp', 'JJ'), ('dive', 'NN'), ('if', 'IN'), ('trade', 'NN'), ('figures', 'NNS'), ('for', 'IN'), ('September', 'NNP'), (',', ','), ('due', 'JJ'), ('for', 'IN'), ('release', 'NN'), ('tomorrow', 'NN'), (',', ','), ('fail', 'VB'), ('to', 'TO'), ('show', 'VB'), ('a', 'DT'), ('substantial', 'JJ'), ('improvement', 'NN'), ('from', 'IN'), ('July', 'NNP'), ('and', 'CC'), ('August', 'NNP'), (\"'s\", 'POS'), ('near-record', 'JJ'), ('deficits', 'NNS'), ('.', '.')], [('Chancellor', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Exchequer', 'NNP'), ('Nigel', 'NNP'), ('Lawson', 'NNP'), (\"'s\", 'POS'), ('restated', 'VBN'), ('commitment', 'NN'), ('to', 'TO'), ('a', 'DT'), ('firm', 'NN'), ('monetary', 'JJ'), ('policy', 'NN'), ('has', 'VBZ'), ('helped', 'VBN'), ('to', 'TO'), ('prevent', 'VB'), ('a', 'DT'), ('freefall', 'NN'), ('in', 'IN'), ('sterling', 'NN'), ('over', 'IN'), ('the', 'DT'), ('past', 'JJ'), ('week', 'NN'), ('.', '.')]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2000.tagged_sents()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for extracting words and their tags from UD annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWordTags(filename):\n",
    "    sents = []\n",
    "    with open(os.path.join(os.getcwd(), filename), 'r', encoding='utf-8') as f:\n",
    "        sent = []\n",
    "        for line in f.readlines():\n",
    "            if line.startswith('# sent_id'):\n",
    "                if sent:\n",
    "                    sents.append(sent)\n",
    "                sent = []\n",
    "            elif line.startswith('#') or line == '\\n':\n",
    "                continue\n",
    "            else:\n",
    "                token_id, token, lemma, pos_class, pos, *other = line.split('\\t')\n",
    "                sent.append((token, pos))\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract words and their POS tags from training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = extractWordTags(r\"data\\UD_English-EWT\\en_ewt-ud-train.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Al', 'NNP'),\n",
       "  ('-', 'HYPH'),\n",
       "  ('Zaman', 'NNP'),\n",
       "  (':', ':'),\n",
       "  ('American', 'JJ'),\n",
       "  ('forces', 'NNS'),\n",
       "  ('killed', 'VBD'),\n",
       "  ('Shaikh', 'NNP'),\n",
       "  ('Abdullah', 'NNP'),\n",
       "  ('al', 'NNP'),\n",
       "  ('-', 'HYPH'),\n",
       "  ('Ani', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('the', 'DT'),\n",
       "  ('preacher', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('mosque', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('town', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('Qaim', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('near', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('Syrian', 'JJ'),\n",
       "  ('border', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('[', '-LRB-'),\n",
       "  ('This', 'DT'),\n",
       "  ('killing', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('respected', 'JJ'),\n",
       "  ('cleric', 'NN'),\n",
       "  ('will', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('causing', 'VBG'),\n",
       "  ('us', 'PRP'),\n",
       "  ('trouble', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('years', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('come', 'VB'),\n",
       "  ('.', '.'),\n",
       "  (']', '-RRB-')],\n",
       " [('DPA', 'NNP'),\n",
       "  (':', ':'),\n",
       "  ('Iraqi', 'JJ'),\n",
       "  ('authorities', 'NNS'),\n",
       "  ('announced', 'VBD'),\n",
       "  ('that', 'IN'),\n",
       "  ('they', 'PRP'),\n",
       "  ('had', 'VBD'),\n",
       "  ('busted', 'VBN'),\n",
       "  ('up', 'RP'),\n",
       "  ('3', 'CD'),\n",
       "  ('terrorist', 'JJ'),\n",
       "  ('cells', 'NNS'),\n",
       "  ('operating', 'VBG'),\n",
       "  ('in', 'IN'),\n",
       "  ('Baghdad', 'NNP'),\n",
       "  ('.', '.')],\n",
       " [('Two', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('them', 'PRP'),\n",
       "  ('were', 'VBD'),\n",
       "  ('being', 'VBG'),\n",
       "  ('run', 'VBN'),\n",
       "  ('by', 'IN'),\n",
       "  ('2', 'CD'),\n",
       "  ('officials', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('Ministry', 'NNP'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('Interior', 'NNP'),\n",
       "  ('!', '.')],\n",
       " [('The', 'DT'),\n",
       "  ('MoI', 'NNP'),\n",
       "  ('in', 'IN'),\n",
       "  ('Iraq', 'NNP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('equivalent', 'JJ'),\n",
       "  ('to', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('US', 'NNP'),\n",
       "  ('FBI', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('so', 'RB'),\n",
       "  ('this', 'DT'),\n",
       "  ('would', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('like', 'IN'),\n",
       "  ('having', 'VBG'),\n",
       "  ('J.', 'NNP'),\n",
       "  ('Edgar', 'NNP'),\n",
       "  ('Hoover', 'NNP'),\n",
       "  ('unwittingly', 'RB'),\n",
       "  ('employ', 'VB'),\n",
       "  ('at', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('high', 'JJ'),\n",
       "  ('level', 'NN'),\n",
       "  ('members', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('Weathermen', 'NNPS'),\n",
       "  ('bombers', 'NNS'),\n",
       "  ('back', 'RB'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('1960s', 'NNS'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare UD tags with NLTK tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e', 'f']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in chain(*[['a','b'], ['c', 'd'], ['e', 'f']])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RB, ., '', VBZ, POS, TO, NN, RBS, NNPS, VBG, VBP, PRP, (, CC, ), JJR, PRP$, ``, FW, $, VB, NNS, PDT, ,, JJS, SYM, RP, UH, VBD, DT, WP$, EX, VBN, JJ, RBR, WRB, WP, IN, MD, WDT, #, :, NNP, CD\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all nltk tags:\n",
    "', '.join(set([i[1] for i in chain(*conll2000.tagged_sents())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"., RB, '', VBZ, POS, TO, ADD, NN, GW, RBS, NNPS, VBG, VBP, LS, PRP, -LRB-, CC, JJR, PRP$, ``, FW, $, VB, NNS, AFX, PDT, XX, NFP, ,, JJS, SYM, RP, UH, VBD, DT, WP$, EX, VBN, HYPH, JJ, RBR, -RRB-, WRB, WP, IN, MD, WDT, :, NNP, CD\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all UD tags:\n",
    "', '.join(set([i[1] for i in chain(*train_data)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tags = set([i[1] for i in chain(*conll2000.tagged_sents())])\n",
    "ud_tags = set([i[1] for i in chain(*train_data)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tags present in NLTK, but not in UD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#', '(', ')'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_tags.difference(ud_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tags present in UD, but not in NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-LRB-', '-RRB-', 'ADD', 'AFX', 'GW', 'HYPH', 'LS', 'NFP', 'XX'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ud_tags.difference(nltk_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy function and normalizer (taken from seminar):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция, которая считает точность.\n",
    "def accuracy(test_sents, postagger):\n",
    "    errors = 0\n",
    "    length = 0\n",
    "    for sent in test_sents:\n",
    "        length += len(sent)\n",
    "        sent, real_tags = zip(*sent)  # что тут произошло?\n",
    "        # предложение (список кортежей) передалось в функцию zip как последовательность кортежей-аргументов\n",
    "        # функция zip вернула два кортежа -  один содержит нулевые элементы кортежей из исходного списка (токены),\n",
    "        # другой содержит первые элементы кортежей из исходного списка (теги)\n",
    "        my_tags = postagger.tag(sent)\n",
    "        for i in range(len(my_tags)):\n",
    "            if my_tags[i][1] != real_tags[i]:\n",
    "                errors += 1\n",
    "    return 1 - errors / length\n",
    "\n",
    "# Нормализатор для получения распределения вероятностей из частот\n",
    "class BaseNormalizer:\n",
    "    def normalize(self, counter):\n",
    "        sum_ = sum(counter.values())\n",
    "        for token in counter:\n",
    "            counter[token] /= sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = zip([1,2,3], ['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'a')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Bigram POS Tagger </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a class that for a given sequence of tokens $w^n_1$ returns sequence of POS tags $t^n_1$, which satisfies the equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$t^n_1 =  \\arg \\max_{t^n_1} \\prod_{i = 1}^{n} P(w_i|t_i) P(t_i|t_{i-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Emission model </b> - stores $P(w_i|t_i)$ scores (taken from seminar)\n",
    "\n",
    "<b> Transition model </b> - stores $P(t_i|t_{i−1})$ scores (taken from seminar and modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmissionModel:\n",
    "    def __init__(self, tagged_sents, normalizer=BaseNormalizer()):\n",
    "        self.normalizer = normalizer\n",
    "        self.model = defaultdict(Counter)\n",
    "        # self.model будет иметь вид \n",
    "        # defaultdict({'tag_1': Counter({'word_1': 0.3, 'word_2': 0.7}), 'tag_2': Counter({'word_1': 0.6, 'word_3': 0.3 ...})})\n",
    "        for sent in tagged_sents:\n",
    "            for word, tag in sent:\n",
    "                self.model[tag][word] += 1\n",
    "        self.add_unk_token()\n",
    "        for tag in self.model:\n",
    "            self.normalizer.normalize(self.model[tag])\n",
    "        \n",
    "    def add_unk_token(self):\n",
    "        # Для каждого тега добавим одинаковую вероятность быть приписанным любому слову, которого нет в модели\n",
    "        for tag in self.model:\n",
    "            self.model[tag]['UNK'] = 0.1\n",
    "        \n",
    "    def tags(self):\n",
    "        # Добавим возможность возвращать все теги, которые есть в модели\n",
    "        return self.model.keys()\n",
    "    \n",
    "    def __getitem__(self, tag):\n",
    "        # Все слова для данного тега\n",
    "        return self.model[tag]\n",
    "    \n",
    "    def __call__(self, word, tag):\n",
    "        # Самое интересное - вероятность P(word|tag)\n",
    "        if word not in self[tag]:\n",
    "            return self[tag]['UNK']\n",
    "        return self[tag][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionModel():\n",
    "    def __init__(self, tag_seqs, normalizer=BaseNormalizer()):\n",
    "        self.normalizer = normalizer\n",
    "        self.model = defaultdict(Counter)\n",
    "        # self.model будет иметь вид \n",
    "        # defaultdict({'tag_1': Counter({'tag_1': 0.3, 'tag_2': 0.7}), 'tag_2': Counter({'tag_1': 0.6, 'tag_3': 0.3 ...})})\n",
    "        for seq in tag_seqs:\n",
    "            for tag_1, tag_2 in zip(['<s>'] + seq[:-1], seq):\n",
    "                self.model[tag_1][tag_2] += 1\n",
    "        for tag_1 in self.model:\n",
    "            self.normalizer.normalize(self.model[tag_1])\n",
    "        \n",
    "    def tags(self):\n",
    "        # Добавим возможность возвращать все теги, которые есть в модели\n",
    "        return self.model.keys()\n",
    "    \n",
    "    def __getitem__(self, tag_1):\n",
    "        # Все теги, которые могут идти после данного тега\n",
    "        return self.model[tag_1]\n",
    "    \n",
    "    def __call__(self, tag_1, tag_2):\n",
    "        # Самое интересное - вероятность P(tag_2|tag_1)\n",
    "        if  tag_2 not in self[tag_1]:\n",
    "            return 0\n",
    "        return self[tag_1][tag_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_argmax(seq=None, func=None, argseq=None):\n",
    "    if seq:\n",
    "        argmax = 0\n",
    "        maximum = seq[argmax]\n",
    "        for i in range(len(seq)):\n",
    "            if seq[i] > maximum:\n",
    "                argmax = i\n",
    "                maximum = seq[argmax]\n",
    "        return maximum, argmax\n",
    "    elif func and argseq:\n",
    "        argmax = argseq[0]\n",
    "        maximum = func(argmax)\n",
    "        for i in argseq:\n",
    "            if func(i) > maximum:\n",
    "                argmax = i\n",
    "                maximum = func(i)\n",
    "        return maximum, argmax\n",
    "    else:\n",
    "        raise ValueError(\"No arguments provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramHMMTagger():\n",
    "    def __init__(self, emission_model, transition_model):\n",
    "        self.em = emission_model\n",
    "        self.tm = transition_model\n",
    "    \n",
    "    def tag(self, sent, return_prob=False):\n",
    "        ## implementation of Viterbi algorithm:\n",
    "        viterbi = []\n",
    "        backpointer = []\n",
    "        tags = list(self.tm.tags())\n",
    "        tags.remove('<s>')\n",
    "        argseq = list(range(len(tags)))\n",
    "        viterbi.append([self.tm('<s>',tag)*self.em(sent[0], tag) for tag in tags])\n",
    "        #print(viterbi)\n",
    "        backpointer.append([None for tag in tags])\n",
    "        for t in range(1, len(sent)):\n",
    "            new_viterbi_row = []\n",
    "            new_backpointer_row = []\n",
    "            for s in range(len(tags)):\n",
    "                m, argm = max_argmax(func = lambda x: viterbi[t-1][x] * self.tm(tags[x], tags[s]) * self.em(sent[t], tags[s]),\n",
    "                                     argseq = argseq)\n",
    "                new_viterbi_row.append(m)\n",
    "                new_backpointer_row.append(argm)\n",
    "            viterbi.append(new_viterbi_row)\n",
    "            backpointer.append(new_backpointer_row)\n",
    "            #print(sent[t])\n",
    "            #print(' '.join([tags[s]+': '+ str(new_viterbi_row[s]) for s in range(len(tags))]))\n",
    "        bestpastprob, bestpathpointer = max_argmax(viterbi[len(sent)-1])\n",
    "        tagged_sent = [None for i in range(len(sent))]\n",
    "        for i in range(len(tagged_sent)-1, -1, -1):\n",
    "            tagged_sent[i] = (sent[i], tags[bestpathpointer])\n",
    "            bestpathpointer = backpointer[i][bestpathpointer]\n",
    "        if return_prob:\n",
    "            return tagged_sent, bestpathprob\n",
    "        return tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = EmissionModel(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = TransitionModel([[tag for word, tag in sent] for sent in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = BigramHMMTagger(em, tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'), ('am', 'VBP'), ('eating', 'VBG'), ('a', 'DT'), ('cake', 'NN')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(['I', 'am', 'eating', 'a', 'cake'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at accuracy on test selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = extractWordTags(r\"data\\UD_English-EWT\\en_ewt-ud-test.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7961478645771025"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(test_data, tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And on train selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8968916957097678"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(train_data, tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at accuracy on conll2000 data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "HMM_acc = lambda data: accuracy(data[8000:],\n",
    "                         BigramHMMTagger(EmissionModel(data[:8000]),\n",
    "                                         TransitionModel([[tag for word, tag in sent] for sent in data[:8000]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8605803867323708"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HMM_acc(conll2000.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
